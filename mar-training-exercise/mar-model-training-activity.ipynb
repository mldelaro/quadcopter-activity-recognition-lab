{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadcopter Activity Recognition - TRAINING YOUR MODEL\n",
    "\n",
    "Welcome to this training module that teaches you how to perform activity recognition with the help of a hosted Machine Learning instance. This notebook will guide you through the process of extracting information from your flight record!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "So let's start with what group name you are... this will help me pull your raw accelerometer data from the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import boto3\n",
    "import boto3.session\n",
    "import string\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "s3_workspace_bucket = 'mldelarosa-thesis'\n",
    "\n",
    "s3_subdir_group_training_flight_log = 'mar-lab-workspace/exercise-training/group-training-dataset/'\n",
    "s3_subdir_group_training_dataset = 'mar-lab-workspace/exercise-training/group-training-dataset/'\n",
    "\n",
    "jupyter_subdir_group_training_dataset = './data/training-group-dataset/'\n",
    "jupyter_subdir_group_training_flight_log = './data/training-group-logs/'\n",
    "jupyter_subdir_group_workspace = './data/group-workspace/'\n",
    "\n",
    "def make_path(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_s3_client():\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource(service_name='s3', verify=True)\n",
    "    return s3.meta.client\n",
    "\n",
    "GROUP_NAME='default'\n",
    "\n",
    "print('[DONE] Runtime Initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [INACTIVE] Lets pull your group's latest Training flight log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NAME = 'default'\n",
    "\n",
    "print ('Pulling latest training data from S3 for [ MAR DATABASE ]');\n",
    "\n",
    "client = get_s3_client()\n",
    "s3_filepath_group_training_dataset_latest = s3_subdir_group_training_dataset + GROUP_NAME + '/imu-data-log-latest';\n",
    "jupyter_filepath_group_training_dataset_latest = jupyter_subdir_group_workspace + GROUP_NAME + '/imu-latest-dataset.csv';\n",
    "\n",
    "print('Downloading from: ' + s3_workspace_bucket + '/' + s3_filepath_group_training_dataset_latest)\n",
    "make_path(jupyter_filepath_group_training_dataset_latest)\n",
    "print('Downloading to: ' + jupyter_filepath_group_training_dataset_latest)\n",
    "group_flight_record = client.download_file(Bucket=s3_workspace_bucket,\n",
    "                                           Key=s3_filepath_group_training_dataset_latest,\n",
    "                                           Filename=jupyter_filepath_group_training_dataset_latest)\n",
    "filepath_group_flight_dataset_latest = jupyter_filepath_group_training_dataset_latest\n",
    "filepath_group_consolidated_flight_dataset = filepath_group_flight_dataset_latest\n",
    "print('[DONE]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [INACTIVE] Let's pull your group's training session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NAME = 'default'\n",
    "\n",
    "print ('Pulling latest flight data from S3 for group [ ' + GROUP_NAME + ' ]');\n",
    "\n",
    "client = get_s3_client()\n",
    "s3_filepath_group_training_flight_log_latest = s3_subdir_group_training_flight_log + GROUP_NAME + '/flight-log-latest';\n",
    "jupyter_filepath_group_training_flight_log_latest = jupyter_subdir_group_workspace + GROUP_NAME + '/flight-log-latest.txt';\n",
    "\n",
    "print('Downloading from: ' + s3_workspace_bucket + '/' + s3_filepath_group_training_flight_log_latest)\n",
    "make_path(jupyter_filepath_group_training_flight_log_latest)\n",
    "print('Downloading to: ' + jupyter_filepath_group_training_flight_log_latest)\n",
    "group_flight_log_record = client.download_file(Bucket=s3_workspace_bucket,\n",
    "                                           Key=s3_filepath_group_training_flight_log_latest,\n",
    "                                           Filename=jupyter_filepath_group_training_flight_log_latest)\n",
    "print('[DONE]')\n",
    "print('========= ' + GROUP_NAME + ' FLIGHT LOG =========')\n",
    "with open(jupyter_filepath_group_training_flight_log_latest, 'r') as flightLog:\n",
    "    print(flightLog.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading MAR Dataset for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download prepped training data\n",
    "s3_filepath_mar_training_database = 'mar-lab-workspace/exercise-training/mar-training-database/imu-database-log-entry-latest'\n",
    "jupyter_filepath_mar_training_database = './data/exercise-training-session/' + GROUP_NAME + '/imu-db/imu-database-training-set.csv'\n",
    "\n",
    "client = get_s3_client()\n",
    "print('Getting from: ' + s3_filepath_mar_training_database)\n",
    "print('Downloading to: ' + jupyter_filepath_mar_training_database)\n",
    "make_path(jupyter_filepath_mar_training_database)\n",
    "group_flight_record = client.download_file(Bucket='mldelarosa-thesis',\n",
    "                                           Key=s3_filepath_mar_training_database,\n",
    "                                           Filename=jupyter_filepath_mar_training_database)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_sets_from_file(data_filepath, destination_dir):\n",
    "    labelled_file = open(data_filepath, 'r')\n",
    "    \n",
    "    csv_columns = ['accelerometer_x','accelerometer_y','accelerometer_z','gyrometer_x','gyrometer_y','gyrometer_z']\n",
    "    \n",
    "    running_index = 0\n",
    "    running_label = ''\n",
    "    running_sample_index = {}\n",
    "    running_sample_filename = ''\n",
    "    labelled_row_reader = csv.DictReader(labelled_file)\n",
    "    for labelled_row in labelled_row_reader:\n",
    "        if(running_label != labelled_row['label']):\n",
    "            # Iterate sample file index for the current label\n",
    "            running_label = labelled_row['label']\n",
    "            if running_label in running_sample_index.keys():\n",
    "                running_sample_index[running_label] = running_sample_index[running_label] + 1;\n",
    "            else:\n",
    "                running_sample_index[running_label] = 0;\n",
    "            running_index = 0;\n",
    "            running_sample_filename = destination_dir + '/' + running_label + '/' + running_label + '-sample-' + str(running_sample_index[running_label]) + '.csv'\n",
    "            make_path(running_sample_filename)\n",
    "            running_sample_file = open(running_sample_filename, 'w')\n",
    "            running_sample_file.write(','.join(csv_columns) + '\\n')\n",
    "            running_sample_file.write(labelled_row['accelerometer_x']\n",
    "                                + ',' + labelled_row['accelerometer_y']\n",
    "                                + ',' + labelled_row['accelerometer_z']\n",
    "                                + ',' + labelled_row['gyrometer_x']\n",
    "                                + ',' + labelled_row['gyrometer_y']\n",
    "                                + ',' + labelled_row['gyrometer_z'] + '\\n')\n",
    "        else:\n",
    "            running_index = running_index + 1\n",
    "            running_sample_file.write(labelled_row['accelerometer_x']\n",
    "                                + ',' + labelled_row['accelerometer_y']\n",
    "                                + ',' + labelled_row['accelerometer_z']\n",
    "                                + ',' + labelled_row['gyrometer_x']\n",
    "                                + ',' + labelled_row['gyrometer_y']\n",
    "                                + ',' + labelled_row['gyrometer_z'] + '\\n')\n",
    "\n",
    "s3_subdir_group_training_session = './data/exercise-training-session/' + GROUP_NAME + '/'\n",
    "extract_label_sets_from_file(jupyter_filepath_mar_training_database, s3_subdir_group_training_session + 'imu-db/')\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "feature_csv_columns = ['average', 'median']\n",
    "imu_data_columns = ['accelerometer_x','accelerometer_y','accelerometer_z','gyrometer_x','gyrometer_y','gyrometer_z']\n",
    "\n",
    "def feature_average(data_sample):\n",
    "    fSum = 0;\n",
    "    nIndex = 0;\n",
    "    for data in data_sample:\n",
    "        fSum = fSum + data\n",
    "        nIndex = nIndex + 1\n",
    "    return float(fSum / nIndex)\n",
    "\n",
    "def feature_variance(data_sample):\n",
    "    return np.var(data_sample)\n",
    "\n",
    "def feature_median(data_sample):\n",
    "    return np.median(data_sample, axis=0)\n",
    "\n",
    "feature_calculations = {\n",
    "    'average' : feature_average,\n",
    "    'median' : feature_median\n",
    "}\n",
    "\n",
    "# Read a *.csv file and extract the sliding window\n",
    "import collections\n",
    "def extract_features_from_imu_data_samples_for_label(data_sample_filepath, features_filepath, data_label):\n",
    "#     print('Extracting for feature: ', data_label, 'from', data_sample_filepath)\n",
    "    with open(data_sample_filepath, 'r') as csv_file:\n",
    "        # extract data records by row\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        sliding_windows = []\n",
    "        sliding_index = 0\n",
    "        window_step_forward = 1\n",
    "        window_length = 4\n",
    "#         if(len(list(reader)) < window_length):\n",
    "#             print('File too short [' + data_sample_filepath + ']')\n",
    "#             return\n",
    "        \n",
    "        # extract sliding windows from rows\n",
    "        sliding_window_csv = []\n",
    "        for row in reader:\n",
    "            sliding_window_csv.append(row)\n",
    "            if(len(sliding_window_csv) == window_length + 1):\n",
    "                del sliding_window_csv[0]\n",
    "            if(sliding_index % window_step_forward == 0 and len(sliding_window_csv) == window_length):\n",
    "                sliding_windows.append(list(sliding_window_csv))\n",
    "            sliding_index = sliding_index + 1\n",
    "        running_window_lines = []\n",
    "                \n",
    "        for feature_name, feature_func in feature_calculations.items():\n",
    "            for imu_data_column in imu_data_columns:\n",
    "                window_sequences = []\n",
    "                for sliding_window in sliding_windows:\n",
    "                    window_sequence = []\n",
    "                    for window in sliding_window:\n",
    "                        window_sequence.append(float(window[imu_data_column]))\n",
    "                    window_sequences.append(window_sequence)\n",
    "#                 print(imu_data_column, ' - ', window_sequences)\n",
    "                \n",
    "                window_index = 0\n",
    "                comma_index = 0;\n",
    "                window_count = len(window_sequences)\n",
    "                while len(running_window_lines) < window_count:\n",
    "                    running_window_lines.append('')\n",
    "                for window in window_sequences:\n",
    "                    running_window_lines[window_index % window_count] += (str(feature_func(window))) + ','\n",
    "#                     print(imu_data_column, ' _ ', feature_name, feature_func(window))\n",
    "                    window_index = window_index + 1\n",
    "#     print('PRINT FOR WINDOW ', window_count , running_window_lines)\n",
    "\n",
    "    with open(features_filepath, 'a') as features_file:\n",
    "        for feature_line in running_window_lines:\n",
    "            features_file.write(data_label + ',' + feature_line[:-1] + '\\n')\n",
    "            \n",
    "\n",
    "# Iterate through the raw IMU data directories and get their labels\n",
    "import glob\n",
    "import os\n",
    "labels = set()\n",
    "print('Extracting labels for samples in raw training data directory:')\n",
    "for raw_data_dir in glob.glob(s3_subdir_group_training_session + 'imu-db' + '/*', recursive=True):\n",
    "    labels.add(os.path.basename(raw_data_dir))\n",
    "print(labels)\n",
    "\n",
    "# labels = set()\n",
    "# print('Extracting labels for samples in raw evaluation data directory:')\n",
    "# for raw_data_dir in glob.glob('./data/evaluation/raw/samples/*', recursive=True):\n",
    "#     labels.add(os.path.basename(raw_data_dir))\n",
    "# print(labels)\n",
    "\n",
    "#os.remove('./data/features.csv')\n",
    "with open(s3_subdir_group_training_session + 'training-data.csv', 'w') as file:\n",
    "    file.write('')\n",
    "\n",
    "# with open('./data/evaluation-feature-data.csv', 'w') as file:\n",
    "#     file.write('')\n",
    "\n",
    "# Don't include first csv row for training files\n",
    "#     running_line = 'label,'\n",
    "#     for feature_columns in feature_csv_columns:\n",
    "#         for imu_data_column in imu_data_columns:\n",
    "#             running_line += feature_columns + '_' + imu_data_column + ','\n",
    "#     running_line = running_line[:-1]\n",
    "#     running_line += '\\n'\n",
    "\n",
    "\n",
    "# Iterate through each raw IMU data sample and extract their features:\n",
    "for labelled_features in labels:\n",
    "    for raw_data_dir in glob.glob(s3_subdir_group_training_session + 'imu-db/' + labelled_features + '/*.csv', recursive=False):\n",
    "        print(\"Extracting features from \" + raw_data_dir)\n",
    "        extract_features_from_imu_data_samples_for_label(raw_data_dir, s3_subdir_group_training_session + 'training-feature-data-latest.csv', labelled_features)\n",
    "\n",
    "# for labelled_features in labels:\n",
    "#     for raw_data_dir in glob.glob('./data/evaluation/raw/samples/' + labelled_features + '/*.csv', recursive=False):\n",
    "# #         print(\"Extracting features from \" + raw_data_dir)\n",
    "#         extract_features_from_imu_data_samples_for_label(raw_data_dir, './data/evaluation-feature-data.csv', labelled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# THIS MODE-DIR POINTS TO THE CLASSIFICATION EXERCISE WORKSPACE!!!!\n",
    "model_dir='../mar-classification-exercise/tmp/model/' + GROUP_NAME + '/'\n",
    "train_data='./data/exercise-training-session/' + GROUP_NAME + '/training-feature-data-latest.csv'\n",
    "eval_data='./data/exercise-training-session/' + GROUP_NAME + '/training-feature-data-latest.csv' # CHANGE ME\n",
    "\n",
    "# delete the model directory\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "# declare feature columns within csv\n",
    "median_gyro_roll = tf.feature_column.numeric_column(key='median_gyro_roll', dtype=tf.float64);\n",
    "median_gyro_pitch = tf.feature_column.numeric_column(key='median_gyro_pitch', dtype=tf.float64);\n",
    "median_gyro_yaw = tf.feature_column.numeric_column(key='median_gyro_yaw', dtype=tf.float64);\n",
    "\n",
    "median_acc_x = tf.feature_column.numeric_column(key='median_acc_x', dtype=tf.float64);\n",
    "median_acc_y = tf.feature_column.numeric_column(key='median_acc_y', dtype=tf.float64);\n",
    "median_acc_z = tf.feature_column.numeric_column(key='median_acc_z', dtype=tf.float64);\n",
    "\n",
    "mean_gyro_roll = tf.feature_column.numeric_column(key='mean_gyro_roll', dtype=tf.float64);\n",
    "mean_gyro_pitch = tf.feature_column.numeric_column(key='mean_gyro_pitch', dtype=tf.float64);\n",
    "mean_gyro_yaw = tf.feature_column.numeric_column(key='mean_gyro_yaw', dtype=tf.float64);\n",
    "\n",
    "# stack feature columns into a single array\n",
    "imu_window_feature_columns = [median_gyro_roll, median_gyro_pitch, median_gyro_yaw,\n",
    "        median_acc_x, median_acc_y, median_acc_z,\n",
    "        mean_gyro_roll, mean_gyro_pitch, mean_gyro_yaw]\n",
    "\n",
    "run_config=tf.estimator.RunConfig().replace(\n",
    "    session_config=tf.ConfigProto(device_count={'GPU': 0})\n",
    ")\n",
    "\n",
    "def input_fn(data_file):\n",
    "    assert tf.gfile.Exists(data_file),('%s not found')\n",
    "    records_default = [['neutral'],\n",
    "                       [0.0], [0.0], [0.0],\n",
    "                       [0.0], [0.0], [0.0],\n",
    "                       [0.0], [0.0], [0.0],\n",
    "                       [0.0], [0.0], [0.0]]\n",
    "    csv_columns = [\n",
    "                    'rotor',\n",
    "                    'mean_acc_x','mean_acc_y','mean_acc_z',\n",
    "                    'mean_gyro_roll','mean_gyro_pitch','mean_gyro_yaw',\n",
    "                    'median_acc_x','median_acc_y','median_acc_z',\n",
    "                    'median_gyro_roll','median_gyro_pitch','median_gyro_yaw'\n",
    "    ]\n",
    "    \n",
    "    def parse_csv(value):\n",
    "        print('PARSING:', data_file)\n",
    "        columns = tf.decode_csv(value, records_default)\n",
    "        features = dict(zip(csv_columns, columns))\n",
    "        labels = features.pop('rotor')\n",
    "        print('LABELS:', labels)\n",
    "        return features, labels\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "    dataset = dataset.shuffle(200)\n",
    "    dataset = dataset.map(parse_csv, 4)\n",
    "    dataset = dataset.batch(200)\n",
    "    return dataset\n",
    "\n",
    "model = tf.estimator.LinearClassifier(\n",
    "    model_dir=model_dir,\n",
    "    feature_columns=imu_window_feature_columns,\n",
    "    config=run_config,\n",
    "    n_classes=5,\n",
    "    label_vocabulary=['backward', 'forward', 'left', 'neutral', 'right']\n",
    ")\n",
    "# model = tf.estimator.DNNClassifier(\n",
    "#     model_dir=model_dir,\n",
    "#     feature_columns=imu_window_feature_columns,\n",
    "#     config=run_config,\n",
    "#     hidden_units=[100, 75, 50, 25],\n",
    "#     n_classes=4,\n",
    "#     label_vocabulary=['1', '2', '3', '4']\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#  <=== Train and evaluate the model every `FLAGS.epochs_per_eval` epochs.  ===>\n",
    "for n in range(40 // 2):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        train_data))\n",
    "\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        eval_data))\n",
    "\n",
    "    # Display evaluation metricshttps://docs.aws.amazon.com/sagemaker/latest/dg/tf-training-inference-code-template.html\n",
    "    print('Results at epoch', (n + 1) * 2)\n",
    "    print('-' * 60)\n",
    "\n",
    "# model.train(input_fn=lambda: input_fn(\n",
    "#     train_data))\n",
    "\n",
    "# results = model.evaluate(input_fn=lambda:input_fn(\n",
    "#     train_data\n",
    "# ))\n",
    "\n",
    "print('[DONE]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
